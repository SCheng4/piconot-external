Evaluation
Author: Emily Blatter, Nick Carter, Sisi Cheng

================================================================================
================================================================================

TODO On a scale of 1-10, we had to change our ideal syntax by ___. TODO

== 10/4 ==

We started by creating our project and setting up repos. We talked
about what we wanted to do and decided to implement our own syntax.
We took a look at our previous grammars and made a few edits to 
create our new grammar ideal. In particular, we kept the `Proof.`
and `QED.` declarations from our grammar actual, but
we went back to using `=` as our assignment symbol instead of
`=>`. Also, we dropped quotation marks and parentheses where
possible to make our syntax more like plain English.

We had some difficulty figuring out how to structure our intermediate 
representation. Though on the surface, our mathbot looks very similar to the
language implemented in external-lab, there are a few differences that make
it much more difficult to implement. For example, the operators on the lhs 
function differently than the operators on the rhs, so we needed to have
a way to show this difference in the ir. Also, each operator is only really
concerned with the thing to the right of it, so that makes our AST more
stick-like. 

== 10/6 ==

We’ve been making some minor tweaks to the language during the 
class work time while constructing our parser. We’re representing 
it less as a two stick tree and adding more branching, so each 
operator has a direction and a rest, which is the rest of that 
side of the rule, which we recurse onto. We also made it so rest 
is now an Option, which allows us to have a base case when 
parsing a rule for when you get to just an operator and a 
direction with no rest.

In our work session after class, we learned that we could pass a 
list of Surroundings instead of a long series of of things 
that take in a direction and a rest recursively. 
This flattens our parse tree considerably and makes things much neater.

We also literally changed a lot of the types of things. We had been
using a structure that  created a tree of types, but we realized 
that some things really aren't the things they were extending. 
As a result, we have switched
to using a base `AST` that we have everything extend instead.
Now we have a rule that seems to parse properly.

Next, we are starting to add the wrapping around our rules that
express our list of rules and looks like a proof. We are realizing
that we will be able to throw out a lot of the tokens in our parse
as the important parts of the program our unambiguously ordered. 
However, we like the extra keyword tokens, so we are going to keep
them in.

We have decided that our base program will be
`Proof. Recall. QED`.

== 10/8 ==

We are trying to match a rule. We first used a really ugly `*` based solution, 
but couldn't figure out what to do with the result. So, we switched to a
built-in function in the parser to handle the repetition. 

== 10/11 ==

Today we started working on the semantics. We started by writing an eval
function that returns a Picobot. However, we decided that when we are dealing
with cases that are not as high-level as Program (for example, Declaration or
Plus), it might be easier to return a piece of a Picobot, like just one rule,
so we created extra eval functions to deal with these that could then be put
together into one whole Picobot at the Program level.

While writing the semantics, how we wanted to deal with users putting in multiple
operators for the same direction on the lhs. We decided that the first 
instruction is what would count unless it’s a *. Basically, as we add new
information about the surroundings, it overwrites the surroundings for
that direction if it is an ‘Anything’, so that way we’re only taking
the most specific instructions.

We also experienced some difficulty with testing for semantics. We found that
there was no equals method for Picobots besides doing it by reference,
so we didn’t have a way to compare test Picobots to what our eval functions
actually returned. We tried to do testing by running the picobots and seeing
if they functioned as expected, but we are having trouble getting them to
run. We’ve posted something on Piazza and hope there is something helpful by
tomorrow.




